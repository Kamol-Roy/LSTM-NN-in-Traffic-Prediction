{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import gzip\n",
    "import eviltransform\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = r\"E:\\Traffic_Prediction_Challenge\"\n",
    "os.chdir(indir)\n",
    "fileList=glob.glob(\"*gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study Area Bounding Box\n",
    "lat1,lon1=eviltransform.wgs2gcj(34.234,108.9415)\n",
    "lat2,lon2=eviltransform.wgs2gcj(34.241,108.943)\n",
    "\n",
    "outfile='Trb_chllng.csv'\n",
    "datas=[]\n",
    "for filename in fileList[:]:    \n",
    "    print('working on :', filename)\n",
    "\n",
    "    df=pd.read_csv(filename,compression='gzip',names=['drID','orID','Time','Lon','Lat'])\n",
    "    df=df[(df['Lat']>=lat1) & (df['Lat']<=lat2)]\n",
    "    df=df[(df['Lon']>=lon1) & (df['Lon']<=lon2)]\n",
    "    datas.append(df)\n",
    "\n",
    "full_df=pd.concat(datas)\n",
    "full_df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    lat1,lon1=eviltransform.gcj2wgs_exact(float(lat1),float(lon1))\n",
    "    lat2,lon2=eviltransform.gcj2wgs_exact(float(lat2),float(lon2))\n",
    "    \n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    # Radius of earth in kilometers is 6371\n",
    "    km = 6371* c\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Trb_chllng.csv')\n",
    "df['Time']=pd.to_datetime(df['Time'],unit='s')+datetime.timedelta(hours=8)\n",
    "df=df.sort_values(by='Time')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month']=df.apply(lambda x: x.Time.month,axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp=df.groupby(by='orID')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df=pd.DataFrame()\n",
    "speed_df['start_time']=grp.Time.first()\n",
    "speed_df['end_time']=grp.Time.last()\n",
    "speed_df['start_lat']=grp.Lat.first()\n",
    "speed_df['end_lat']=grp.Lat.last()\n",
    "speed_df['start_lon']=grp.Lon.first()\n",
    "speed_df['end_lon']=grp.Lon.last()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df['month']=grp.month.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df['time_diff']=speed_df.apply(lambda x: (x['end_time']-x['start_time']).total_seconds()/3600, axis=1)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df['distance']=speed_df.apply(lambda x: haversine(x.start_lon, x.start_lat, x.end_lon, x.end_lat), axis=1 )\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df=speed_df[speed_df['distance']>0]\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df['direction']=speed_df.apply(lambda x: 'North' if x.end_lat>x.start_lat else 'South', axis=1)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df['speed']=speed_df['distance']/speed_df['time_diff']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod=speed_df\n",
    "df_mod_N=df_mod[df_mod['direction']=='North']\n",
    "df_mod_S=df_mod[df_mod['direction']=='South']\n",
    "\n",
    "df_mod_NO=df_mod_N[df_mod_N['month']==10]\n",
    "df_mod_NO=df_mod_NO.resample(rule='5Min',on='start_time' ).mean()\n",
    "df_mod_NO=df_mod_NO.reset_index()\n",
    "\n",
    "df_mod_NN=df_mod_N[df_mod_N['month']==11]\n",
    "df_mod_NN=df_mod_NN.resample(rule='5Min',on='start_time' ).mean()\n",
    "df_mod_NN=df_mod_NN.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "df_mod_SO=df_mod_S[df_mod_S['month']==10]\n",
    "df_mod_SO=df_mod_SO.resample(rule='5Min',on='start_time' ).mean()\n",
    "df_mod_SO=df_mod_SO.reset_index()\n",
    "\n",
    "df_mod_SN=df_mod_S[df_mod_S['month']==11]\n",
    "df_mod_SN=df_mod_SN.resample(rule='5Min',on='start_time' ).mean()\n",
    "df_mod_SN=df_mod_SN.reset_index()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(2,1,figsize=(12,7))\n",
    "axs[0].plot(df_mod_NO['start_time'],df_mod_NO['speed'],label='North Direction')\n",
    "axs[0].plot(df_mod_SO['start_time'],df_mod_SO['speed'],label='South Direction')\n",
    "\n",
    "axs[1].plot(df_mod_NN['start_time'],df_mod_NN['speed'],label='North Direction')\n",
    "axs[1].plot(df_mod_SN['start_time'],df_mod_SN['speed'],label='South Direction')\n",
    "\n",
    "\n",
    "axs[0].xaxis.set_major_formatter(mdates.DateFormatter('%m-%d \\n%H:%M %p'))\n",
    "axs[0].xaxis.set_major_locator(mticker.MaxNLocator(15)) \n",
    "\n",
    "axs[1].xaxis.set_major_formatter(mdates.DateFormatter('%m-%d \\n%H:%M %p'))\n",
    "axs[1].xaxis.set_major_locator(mticker.MaxNLocator(15)) \n",
    "\n",
    "axs[0].set_ylabel('Speed (kmph)')\n",
    "axs[0].legend(loc='upper right')\n",
    "\n",
    "\n",
    "axs[1].set_ylabel('Speed (kmph)')\n",
    "axs[1].legend(loc='upper right')\n",
    "\n",
    "axs[0].set_xlabel('(a) For October, 2016',weight='bold')\n",
    "axs[1].set_xlabel('(b) For November, 2016', weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('5minplot.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_mean=df_mod_N.groupby(by=[df_mod_N.start_time.map(lambda x : x.hour)]).speed.mean()\n",
    "grp_std=df_mod_N.groupby(by=[df_mod_N.start_time.map(lambda x : x.hour)]).speed.std()\n",
    "\n",
    "\n",
    "grp_mean_S=df_mod_S.groupby(by=[df_mod_S.start_time.map(lambda x : x.hour)]).speed.mean()\n",
    "grp_std_S=df_mod_S.groupby(by=[df_mod_S.start_time.map(lambda x : x.hour)]).speed.std()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = pd.date_range('2017-04-03 00:00:00', periods=24, freq='H')\n",
    "df = pd.DataFrame({'starttime': rng})  \n",
    "cats = df.starttime.dt.strftime('%I %p').tolist()\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(10,7))\n",
    "\n",
    "alpha=.4\n",
    "ax.errorbar(range(24),grp_mean,yerr=grp_std,color='g',alpha=alpha,label='Standard Error(N)')\n",
    "ax.plot(range(24),grp_mean,color='g',marker='d',markersize=10,label='North Direction(N)')\n",
    "\n",
    "ax.errorbar(range(24),grp_mean_S,yerr=grp_std_S,color='r',alpha=alpha,label='Standard Error(S)')\n",
    "ax.plot(range(24),grp_mean_S,color='r',marker='o',markersize=10,label='South Direction(S)')\n",
    "ax.set_ylabel('Speed (kmph)')\n",
    "\n",
    "tlabels=[]\n",
    "for i in range(24):\n",
    "    if i % 2==0:\n",
    "        tlabels.append(cats[i])\n",
    "    else:\n",
    "        tlabels.append(' ')\n",
    "plt.xticks(range(24),tlabels)\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_plot.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod_N.to_csv('model_data_N.csv')\n",
    "df_mod_S.to_csv('model_data_S.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Models For North Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod_N=pd.read_csv('model_data_N.csv')\n",
    "df_mod_N.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod_N=df_mod_N[['start_time','speed']]\n",
    "df_mod_N['start_time']=pd.to_datetime(df_mod_N['start_time'])\n",
    "df_mod_N.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod_N5min=df_mod_N.resample(rule='5Min',on='start_time' ).mean()\n",
    "df_mod_N5min=df_mod_N5min.reset_index()\n",
    "df_mod_N5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Creation\n",
    "df_mod_N5min['hour']=df_mod_N5min.apply(lambda x: x.start_time.hour,axis=1)\n",
    "df_mod_N5min['minute']=df_mod_N5min.apply(lambda x: x.start_time.minute,axis=1)\n",
    "df_mod_N5min.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaler= MinMaxScaler(feature_range=(0, 1))\n",
    "#scaler=StandardScaler()\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras import optimizers\n",
    "from sklearn import model_selection\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import neighbors\n",
    "from math import sqrt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_List=[]\n",
    "RMSEs=[]\n",
    "for i in range(1,5*12+1):\n",
    "    print('Working On Model : ', i)\n",
    "    \n",
    "    df_mod_N5min['lag1_speed']=df_mod_N5min['speed'].shift(i)\n",
    "    df_mod_N5min['lag2_speed']=df_mod_N5min['speed'].shift(i+1)\n",
    "    df_mod_N5min['lag3_speed']=df_mod_N5min['speed'].shift(i+2)\n",
    "    df_mod_N5min['lag4_speed']=df_mod_N5min['speed'].shift(i+3)\n",
    "    df_mod_N5min['lag5_speed']=df_mod_N5min['speed'].shift(i+4) \n",
    "    df_mod_N5min['lag6_speed']=df_mod_N5min['speed'].shift(i+5) \n",
    "    \n",
    "    df_comb=df_mod_N5min[['start_time','hour','minute','lag1_speed','lag2_speed','lag3_speed','lag4_speed','lag5_speed','lag6_speed','speed']]\n",
    "    df_comb=df_comb.dropna()\n",
    "    df_comb_np=df_comb.values[:]\n",
    "    test_size=int(.2*len(df_comb_np))\n",
    "\n",
    "    times=df_comb_np[:,0]\n",
    "\n",
    "    train_time=times[:-test_size]\n",
    "    test_time=times[-test_size:]\n",
    "\n",
    "    df_comb_np=df_comb_np[:,1:]\n",
    "\n",
    "    df_comb_np_scaled=scaler.fit_transform(df_comb_np)\n",
    "\n",
    "    X=df_comb_np_scaled[:,:-1]\n",
    "    y=df_comb_np_scaled[:,-1]\n",
    "    #x_train,x_test,y_train,y_test=model_selection.train_test_split(X, y, test_size=.2, random_state=50)\n",
    "    train=df_comb_np_scaled[:-test_size]\n",
    "    test=df_comb_np_scaled[-test_size:]\n",
    "\n",
    "    x_train = train[:,:-1]\n",
    "    x_test=test[:,:-1]\n",
    "    y_train=train[:,-1]\n",
    "    y_test=test[:,-1]\n",
    "\n",
    "\n",
    "    x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "    x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "    epoc_size=25\n",
    "    early_stopping_monitor = EarlyStopping(patience=1,monitor='val_loss',\n",
    "                                  min_delta=0,\n",
    "                                  verbose=0, mode='auto')\n",
    "\n",
    "    model = Sequential()\n",
    "    #model.add(LSTM(500,return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(LSTM(500, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "\n",
    "    opt=optimizers.adam(lr=.0001)\n",
    "    #model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1,activation='tanh'))\n",
    "    model.compile(loss='mse', optimizer=opt)\n",
    "    # fit network\n",
    "    #history = model.fit(x_train, y_train,batch_size=10, validation_data=(x_test, y_test), verbose=2, shuffle=True)\n",
    "\n",
    "    history=model.fit(x_train, y_train,epochs=epoc_size,batch_size=6, validation_data=(x_test, y_test), verbose=1,callbacks=[early_stopping_monitor], shuffle=False)\n",
    "    print('done')\n",
    "    \n",
    "    Model_List.append(model)\n",
    "\n",
    "    # plot history\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    test_X=x_train\n",
    "    test_y=y_train\n",
    "\n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = np.concatenate((test_X[:, :],yhat), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,-1]\n",
    "    train_yhat=inv_yhat\n",
    "\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = np.concatenate((test_X[:, :],test_y), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,-1]\n",
    "    # calculate RMSE\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    print('Train RMSE: %.3f' % rmse)\n",
    "\n",
    "\n",
    "    test_X=x_test\n",
    "    test_y=y_test\n",
    "\n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = np.concatenate((test_X[:, :],yhat), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,-1]\n",
    "    test_yhat=inv_yhat\n",
    "\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = np.concatenate((test_X[:, :],test_y), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,-1]\n",
    "    # calculate RMSE\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    RMSEs.append(rmse)\n",
    "    fig=plt.figure(figsize=(15,7))\n",
    "    plt.plot(range(len(inv_yhat[:100])),inv_yhat[:100],label='prediction')\n",
    "    plt.plot(range(len(inv_y[:100])),inv_y[:100],label='actual')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,6))\n",
    "plt.plot(np.arange(1,len(RMSEs)+1)*5, np.array(RMSEs))\n",
    "plt.xlabel('Forecast Horizon (Minute)')\n",
    "plt.ylabel('RMSE (kmph)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data=\"Predictions_north.csv\"\n",
    "pred_df=pd.read_csv(pred_data)\n",
    "len(pred_df)\n",
    "\n",
    "df_comb_np_scaled.shape\n",
    "\n",
    "pred_df['time_dt']=pd.to_datetime(pred_df['time'])\n",
    "\n",
    "pred_df=pred_df.replace('x',10**6)\n",
    "print(len(pred_df))\n",
    "pred_df['speed']=pred_df['speed'].astype(float)\n",
    "\n",
    "pred_df=pred_df.set_index('time_dt').resample('5Min').ffill().reset_index()\n",
    "pred_df=pred_df.replace(10**6,np.nan)\n",
    "\n",
    "fig=plt.subplots(figsize=(10,7))\n",
    "plt.plot(pred_df['time_dt'],list(pred_df['speed']))\n",
    "plt.show()\n",
    "\n",
    "Predict_df=pred_df\n",
    "\n",
    "Predict_df['hour']=Predict_df.apply(lambda x: x.time_dt.hour,axis=1)\n",
    "Predict_df['minute']=Predict_df.apply(lambda x: x.time_dt.minute,axis=1)\n",
    "\n",
    "Predict_df['lag1_speed']=Predict_df['speed']\n",
    "Predict_df['lag2_speed']=Predict_df['speed'].shift(1)\n",
    "Predict_df['lag3_speed']=Predict_df['speed'].shift(2)\n",
    "Predict_df['lag4_speed']=Predict_df['speed'].shift(3)\n",
    "Predict_df['lag5_speed']=Predict_df['speed'].shift(4)\n",
    "Predict_df['lag6_speed']=Predict_df['speed'].shift(5)\n",
    "Predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=Predict_df[['hour','minute','lag1_speed','lag2_speed','lag3_speed','lag4_speed','lag5_speed','lag6_speed','speed']].loc[72].values\n",
    "test_scaled=scaler.transform(test.reshape(1,9))[:,:-1]\n",
    "f_test_scaled_r=test_scaled.reshape(1,1,test_scaled.shape[1])\n",
    "\n",
    "test=Predict_df[['hour','minute','lag1_speed','lag2_speed','lag3_speed','lag4_speed','lag5_speed','lag6_speed','speed']].loc[192].values\n",
    "test_scaled=scaler.transform(test.reshape(1,9))[:,:-1]\n",
    "s_test_scaled_r=test_scaled.reshape(1,1,test_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_time=Predict_df.loc[72]['time_dt']\n",
    "s_time=Predict_df.loc[192]['time_dt']\n",
    "\n",
    "def find_index(x,f_time,s_time):\n",
    "    time_diff1=(x.time_dt-f_time).total_seconds()/(60*5)\n",
    "    time_diff2=(x.time_dt-s_time).total_seconds()/(60*5)\n",
    "    if 1<=time_diff1 <61:\n",
    "        index=time_diff1\n",
    "        \n",
    "    elif 0<time_diff2 <61:\n",
    "       index=-time_diff2\n",
    "    else:\n",
    "        index=-99999\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_df['model_index']=Predict_df.apply(lambda x: find_index(x,f_time,s_time), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model_index,time_stamp, f_test, s_test,Model_List):\n",
    "    model_index=int(model_index)\n",
    "    hour=time_stamp.hour\n",
    "    minute=time_stamp.minute\n",
    "    h_scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    h_scaler.fit_transform(np.array(Predict_df['hour']).reshape(-1,1))\n",
    "    \n",
    "    m_scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    m_scaler.fit_transform(np.array(Predict_df['minute']).reshape(-1,1))\n",
    "    hour=h_scaler.transform(np.array(hour).reshape(-1,1))\n",
    "    minute=m_scaler.transform(np.array(minute).reshape(-1,1))\n",
    "    f_test_mod=f_test.reshape(1,8)\n",
    "    f_test_mod[:,0]=hour\n",
    "    f_test_mod[:,1]=minute\n",
    "    f_test=f_test_mod.reshape(1,1,8)\n",
    "    \n",
    "    s_test_mod=s_test.reshape(1,8)\n",
    "    s_test_mod[:,0]=hour\n",
    "    s_test_mod[:,1]=minute\n",
    "    s_test=s_test_mod.reshape(1,1,8)\n",
    "    \n",
    "    \n",
    " #   print(model_index)\n",
    "    if model_index in range(1,61):\n",
    "        pred_val=Model_List[model_index-1].predict(f_test)\n",
    "        pred_val=new_scaler.inverse_transform(pred_val)[0][0]\n",
    "    elif model_index in range(-61,0):\n",
    "    #    print('mod',-model_index-1)\n",
    "        pred_val=Model_List[-(model_index)-1].predict(s_test)\n",
    "        pred_val=new_scaler.inverse_transform(pred_val)[0][0]\n",
    "    else:\n",
    "        pred_val=np.nan\n",
    "    return pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "Predict_df['prediction']=Predict_df.apply(lambda x: make_prediction(x.model_index,x.time_dt,f_test_scaled_r,s_test_scaled_r, Model_List), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(20,7))\n",
    "ax.plot(pred_df['time_dt'],list(pred_df['speed']),label='Given Speed Data')\n",
    "ax.plot(Predict_df['time_dt'],list(Predict_df['prediction']),label='Prediction')\n",
    "ax.set_xlim(pred_df['time_dt'].min(),pred_df['time_dt'].max())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M %p'))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_df_N=Predict_df[['time','speed','prediction']]\n",
    "\n",
    "Predict_df_N.to_csv('North_data_Prediction.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(pred_df,Predict_df_N, how='left', on='time')\n",
    "\n",
    "df=df.drop_duplicates(subset='time', keep=\"first\")\n",
    "\n",
    "df[['time','speed','prediction']].to_csv('Prediction_north_updated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Models For South Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod_S=pd.read_csv('model_data_S.csv')\n",
    "df_mod_S.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod_S=df_mod_S[['start_time','speed']]\n",
    "df_mod_S['start_time']=pd.to_datetime(df_mod_S['start_time'])\n",
    "df_mod_S.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod_S5min=df_mod_S.resample(rule='5Min',on='start_time' ).mean()\n",
    "df_mod_S5min=df_mod_S5min.reset_index()\n",
    "df_mod_S5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Creation\n",
    "df_mod_S5min['hour']=df_mod_S5min.apply(lambda x: x.start_time.hour,axis=1)\n",
    "df_mod_S5min['minute']=df_mod_S5min.apply(lambda x: x.start_time.minute,axis=1)\n",
    "df_mod_S5min.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaler= MinMaxScaler(feature_range=(0, 1))\n",
    "#scaler=StandardScaler()\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras import optimizers\n",
    "from sklearn import model_selection\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import neighbors\n",
    "from math import sqrt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_List=[]\n",
    "RMSEs=[]\n",
    "for i in range(1,5*12+1):\n",
    "    print('Working On Model : ', i)\n",
    "    \n",
    "    df_mod_S5min['lag1_speed']=df_mod_S5min['speed'].shift(i)\n",
    "    df_mod_S5min['lag2_speed']=df_mod_S5min['speed'].shift(i+1)\n",
    "    df_mod_S5min['lag3_speed']=df_mod_S5min['speed'].shift(i+2)\n",
    "    df_mod_S5min['lag4_speed']=df_mod_S5min['speed'].shift(i+3)\n",
    "    df_mod_S5min['lag5_speed']=df_mod_S5min['speed'].shift(i+4) \n",
    "    df_mod_S5min['lag6_speed']=df_mod_S5min['speed'].shift(i+5) \n",
    "    \n",
    "    df_comb=df_mod_S5min[['start_time','hour','minute','lag1_speed','lag2_speed','lag3_speed','lag4_speed','lag5_speed','lag6_speed','speed']]\n",
    "    df_comb=df_comb.dropna()\n",
    "    df_comb_np=df_comb.values[:]\n",
    "    test_size=int(.2*len(df_comb_np))\n",
    "\n",
    "    times=df_comb_np[:,0]\n",
    "\n",
    "    train_time=times[:-test_size]\n",
    "    test_time=times[-test_size:]\n",
    "\n",
    "    df_comb_np=df_comb_np[:,1:]\n",
    "\n",
    "    df_comb_np_scaled=scaler.fit_transform(df_comb_np)\n",
    "\n",
    "    X=df_comb_np_scaled[:,:-1]\n",
    "    y=df_comb_np_scaled[:,-1]\n",
    "    #x_train,x_test,y_train,y_test=model_selection.train_test_split(X, y, test_size=.2, random_state=50)\n",
    "    train=df_comb_np_scaled[:-test_size]\n",
    "    test=df_comb_np_scaled[-test_size:]\n",
    "\n",
    "    x_train = train[:,:-1]\n",
    "    x_test=test[:,:-1]\n",
    "    y_train=train[:,-1]\n",
    "    y_test=test[:,-1]\n",
    "\n",
    "\n",
    "    x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "    x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "    epoc_size=25\n",
    "    early_stopping_monitor = EarlyStopping(patience=1,monitor='val_loss',\n",
    "                                  min_delta=0,\n",
    "                                  verbose=0, mode='auto')\n",
    "\n",
    "    model = Sequential()\n",
    "    #model.add(LSTM(500,return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(LSTM(500, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "\n",
    "    opt=optimizers.adam(lr=.0001)\n",
    "    #model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1,activation='tanh'))\n",
    "    model.compile(loss='mse', optimizer=opt)\n",
    "    # fit network\n",
    "    #history = model.fit(x_train, y_train,batch_size=10, validation_data=(x_test, y_test), verbose=2, shuffle=True)\n",
    "\n",
    "    history=model.fit(x_train, y_train,epochs=epoc_size,batch_size=6, validation_data=(x_test, y_test), verbose=1,callbacks=[early_stopping_monitor], shuffle=False)\n",
    "    print('done')\n",
    "    \n",
    "    Model_List.append(model)\n",
    "\n",
    "    # plot history\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    test_X=x_train\n",
    "    test_y=y_train\n",
    "\n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = np.concatenate((test_X[:, :],yhat), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,-1]\n",
    "    train_yhat=inv_yhat\n",
    "\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = np.concatenate((test_X[:, :],test_y), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,-1]\n",
    "    # calculate RMSE\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    print('Train RMSE: %.3f' % rmse)\n",
    "\n",
    "\n",
    "    test_X=x_test\n",
    "    test_y=y_test\n",
    "\n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = np.concatenate((test_X[:, :],yhat), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,-1]\n",
    "    test_yhat=inv_yhat\n",
    "\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = np.concatenate((test_X[:, :],test_y), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,-1]\n",
    "    # calculate RMSE\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    RMSEs.append(rmse)\n",
    "    fig=plt.figure(figsize=(15,7))\n",
    "    plt.plot(range(len(inv_yhat[:100])),inv_yhat[:100],label='prediction')\n",
    "    plt.plot(range(len(inv_y[:100])),inv_y[:100],label='actual')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,6))\n",
    "plt.plot(np.arange(1,len(RMSEs)+1)*5, np.array(RMSEs))\n",
    "plt.xlabel('Forecast Horizon (Minute)')\n",
    "plt.ylabel('RMSE (kmph)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data=\"Predictions_south.csv\"\n",
    "pred_df=pd.read_csv(pred_data)\n",
    "len(pred_df)\n",
    "\n",
    "df_comb_np_scaled.shape\n",
    "\n",
    "pred_df['time_dt']=pd.to_datetime(pred_df['time'])\n",
    "\n",
    "pred_df=pred_df.replace('x',10**6)\n",
    "print(len(pred_df))\n",
    "pred_df['speed']=pred_df['speed'].astype(float)\n",
    "\n",
    "pred_df=pred_df.set_index('time_dt').resample('5Min').ffill().reset_index()\n",
    "pred_df=pred_df.replace(10**6,np.nan)\n",
    "\n",
    "fig=plt.subplots(figsize=(10,7))\n",
    "plt.plot(pred_df['time_dt'],list(pred_df['speed']))\n",
    "plt.show()\n",
    "\n",
    "Predict_df=pred_df\n",
    "\n",
    "Predict_df['hour']=Predict_df.apply(lambda x: x.time_dt.hour,axis=1)\n",
    "Predict_df['minute']=Predict_df.apply(lambda x: x.time_dt.minute,axis=1)\n",
    "\n",
    "Predict_df['lag1_speed']=Predict_df['speed']\n",
    "Predict_df['lag2_speed']=Predict_df['speed'].shift(1)\n",
    "Predict_df['lag3_speed']=Predict_df['speed'].shift(2)\n",
    "Predict_df['lag4_speed']=Predict_df['speed'].shift(3)\n",
    "Predict_df['lag5_speed']=Predict_df['speed'].shift(4)\n",
    "Predict_df['lag6_speed']=Predict_df['speed'].shift(5)\n",
    "Predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=Predict_df[['hour','minute','lag1_speed','lag2_speed','lag3_speed','lag4_speed','lag5_speed','lag6_speed','speed']].loc[72].values\n",
    "test_scaled=scaler.transform(test.reshape(1,9))[:,:-1]\n",
    "f_test_scaled_r=test_scaled.reshape(1,1,test_scaled.shape[1])\n",
    "\n",
    "test=Predict_df[['hour','minute','lag1_speed','lag2_speed','lag3_speed','lag4_speed','lag5_speed','lag6_speed','speed']].loc[192].values\n",
    "test_scaled=scaler.transform(test.reshape(1,9))[:,:-1]\n",
    "s_test_scaled_r=test_scaled.reshape(1,1,test_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_time=Predict_df.loc[72]['time_dt']\n",
    "s_time=Predict_df.loc[192]['time_dt']\n",
    "\n",
    "def find_index(x,f_time,s_time):\n",
    "    time_diff1=(x.time_dt-f_time).total_seconds()/(60*5)\n",
    "    time_diff2=(x.time_dt-s_time).total_seconds()/(60*5)\n",
    "    if 1<=time_diff1 <61:\n",
    "        index=time_diff1\n",
    "        \n",
    "    elif 0<time_diff2 <61:\n",
    "       index=-time_diff2\n",
    "    else:\n",
    "        index=-99999\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_df['model_index']=Predict_df.apply(lambda x: find_index(x,f_time,s_time), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model_index,time_stamp, f_test, s_test,Model_List):\n",
    "    model_index=int(model_index)\n",
    "    hour=time_stamp.hour\n",
    "    minute=time_stamp.minute\n",
    "    h_scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    h_scaler.fit_transform(np.array(Predict_df['hour']).reshape(-1,1))\n",
    "    \n",
    "    m_scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    m_scaler.fit_transform(np.array(Predict_df['minute']).reshape(-1,1))\n",
    "    hour=h_scaler.transform(np.array(hour).reshape(-1,1))\n",
    "    minute=m_scaler.transform(np.array(minute).reshape(-1,1))\n",
    "    f_test_mod=f_test.reshape(1,8)\n",
    "    f_test_mod[:,0]=hour\n",
    "    f_test_mod[:,1]=minute\n",
    "    f_test=f_test_mod.reshape(1,1,8)\n",
    "    \n",
    "    s_test_mod=s_test.reshape(1,8)\n",
    "    s_test_mod[:,0]=hour\n",
    "    s_test_mod[:,1]=minute\n",
    "    s_test=s_test_mod.reshape(1,1,8)\n",
    "    \n",
    "    \n",
    " #   print(model_index)\n",
    "    if model_index in range(1,61):\n",
    "        pred_val=Model_List[model_index-1].predict(f_test)\n",
    "        pred_val=new_scaler.inverse_transform(pred_val)[0][0]\n",
    "    elif model_index in range(-61,0):\n",
    "    #    print('mod',-model_index-1)\n",
    "        pred_val=Model_List[-(model_index)-1].predict(s_test)\n",
    "        pred_val=new_scaler.inverse_transform(pred_val)[0][0]\n",
    "    else:\n",
    "        pred_val=np.nan\n",
    "    return pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_df['prediction']=Predict_df.apply(lambda x: make_prediction(x.model_index,x.time_dt,f_test_scaled_r,s_test_scaled_r, Model_List), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(20,7))\n",
    "ax.plot(pred_df['time_dt'],list(pred_df['speed']),label='Given Speed Data')\n",
    "ax.plot(Predict_df['time_dt'],list(Predict_df['prediction']),label='Prediction')\n",
    "ax.set_xlim(pred_df['time_dt'].min(),pred_df['time_dt'].max())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M %p'))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_df_S=Predict_df[['time','speed','prediction']]\n",
    "\n",
    "Predict_df_S.to_csv('South_data_Prediction.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(pred_df,Predict_df_S, how='left', on='time')\n",
    "\n",
    "df=df.drop_duplicates(subset='time', keep=\"first\")\n",
    "\n",
    "df[['time','speed','prediction']].to_csv('Prediction_south_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cheers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
